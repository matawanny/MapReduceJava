CUSIP|Issue_Date|Pool_Correction_Indicator|As_of_Date|Pool_Number|Loan_Correction_Indicator|Loan_Identifier|Channel|Seller_Name|Service_Name|Original_Interest_Rate|Current_Interest_Rate|Current_Net_Interest_Rate|Original_UPB|Current_UPB|Original_Loan_Term|First_Payment_Date|Loan_Age|Remaining_Months_to_Maturity|Maturity_Date|LTV|CLTV|Number_Of_Borrowers|Debt_to_Income_Ratio|Credit_Score|First_Time_Home_Buyer_Indicator|Loan_Purpose|Property_Type|Numbers_of_Units|Occupancy_Status|State|Mortgage_Insurance_Percentage|Product_Type|Prepayment_Premium_Term|Interest_Only_Indicator|First_P_I_Payment_Date|Months_to_First_Scheduled_Amortization|Convertibility_Indicator|Mortgage_Margin|Net_Mortgage_Margin|Index|Interest_Rate_Look_Back|Max_Interest_Rate|Net_Max_Interest_Rate|Months_to_Next_Rate_Change|Next_Rate_Change_Date|Rate_Adjustment_Frequency|Initial_Fixed_Rate_Period|Initial_Rate_Cap_Up_Percent|Initial_Rate_Cap_Down_Percent|Periodic_Cap_Up_Percernt|Periodic_Cap_Down_Percernt|Days_Delinquent|Loan_Performance_History|Loan_Age_as_of_Modification|Modificaiton_Program|Modification_Type|Number_Of_Modifications|Total_Capitalized_Amount|Original_Mortgage_Loan_UPB|f1|Current_Deferred_UPB|Interest_Rate_Step_Indicator|Initial_Step_Fixed_Rate_Period|Total_Number_of_Steps|Number_of_Remaining_Steps|Next_Step_Rate|Terminal_Step_Rate|Date_Of_Terminal_Step|Step_Rate_Adjustment_Frequencey|Months_to_Next_Step_Rate_Change|Next_Step_Rate_Change_Date|Periodic_Step_Cap_Up_Percent|Origination_Channel|Origination_Interest_Rate|Origination_UPB|Origination_Loan_Term|Origination_First_Payment_Date|Origination_Maturity_Date|Origination_LTV|Origination_CLTV|Origination_Debt_To_Income_Ratio|Origination_Credit_Score|Origination_Loan_Purpose|Origination_Occupancy_Status|Origination_Product_Type|Origination_Interest_Only_Indicator


source file should be 84

/cygdrive/C/data$ head -n 1 FNMMONLL.TXT | awk -F\| '{print NF}'
87
/cygdrive/C/data$ head -n 2 FNMMONLL.TXT | tail -n 1| awk -F\| '{print NF}'
87
/cygdrive/C/data$ head -n 2 FNMMONLL.TXT | tail -n 1



kite-dataset csv-schema FNMMONLL.TXT --record-name fnma_month_loan --delimiter '\|' --require cusip,issue_date,as_of_date,pool_number,loan_identifier -o FNMAMONLY.avsc



kite-dataset create dataset:hive://ybrdev79:9083/prd/fnmamonly --schema fnma_monthly.avsc --partition-by year-month-fnmamonly.json --format parquet

kite-dataset info dataset:hive://ybrdev79:9083/prd/fnmamonly

kite-dataset csv-import FNMMONLL.TXT dataset:hive://ybrdev79:9083/prd/fnmamonly --delimiter '|' --no-header

kite-dataset delete dataset:hive://ybrdev79:9083/prd/fnmamonly

unset SSH_ASKPASS
git config --global http.sslVerify false
git clone https://FX36019@ybgweb.ny.ssmb.com/ybgit/scm/bdetl/yb-bigdata.git

git remote add origin https://FX36019@ybgweb.ny.ssmb.com/ybgit/scm/bdetl/yb-bigdata.git

git push -u origin master  

create table prd.fnma_monitors(file_name String,  file_size bigint,  file_lines bigint,  as_of_date bigint, year int, month int,  start_time bigint,  process_last_seconds bigint, total_records bigint, status string)



hadoop fs -ls /user/hive/warehouse/prd.db/fnmamonly/year=2018/month=04
hadoop fs -rm /user/hive/warehouse/prd.db/fnmamonly/year=2018/month=04/*

git fetch --all
git reset --hard origin/master



git reset --hard HEAD
git pull

/net/ybr-prodnfs11/embs_prod/PREV_MONTH/
/net/ybr-prodnfs11/vendordata-PROD/data-grp15/embsdata/embs/daily/

sh -x /usr/book/app/yb-bigdata/src/main/scripts/fnma/PreprocessFiles.sh FNMMONLL.ZIP

Switch Java Environment in Linux box.
Sqoop require Java 1.7 while embs application require Java 1.8

Refer to https://tecadmin.net/install-java-8-on-centos-rhel-and-fedora/

1. update $JAVA_HOME /etc/bashrc
2. #alternatives --config java

re-login 

sqoop require encrypted password

hadoop credential create uat.password.alias -provider jceks://hdfs/user/embs/uat.password.jceks
Enter password:
bpa123

kite-dataset update dataset:hive://ybrdev79:9083/prd/fnmamonly --schema fnma_monthly.avsc


alter table prd.monitors SET TBLPROPERTIES('serialization.null.format'='');

kite-dataset delete dataset:hive://ybrdev79:9083/fnma_loan_daily
kite-dataset create dataset:hive://ybrdev79:9083/fnma_loan_daily --schema fnma_loan_daily.avsc --partition-by fnma_daily_partition.json --format parquet
kite-dataset create dataset:hive://ybrdev79:9083/fnma_arm_loan_daily --schema fnma_arm_loan_daily.avsc --partition-by fnma_daily_partition.json --format parquet
kite-dataset create dataset:hive://ybrdev79:9083/fnma_mod_loan_daily --schema fnma_mod_loan_daily.avsc --partition-by fnma_daily_partition.json --format parquet

____________________
OOZIE
serially/dependent
in parallel/independent
Directed Acyclic Graph (no circle) (DAG)
-- workflow
A (OOZIE) workflow specifies a set of actions and the order and conditions under which those actions should be performed.

Actions: MapReduce Job/Hive Query/Pig Query/Shell Scripts/Java Program

Each of these are individual workflows by themselves

THe coordinator schedules the execution of a workflow at a specified time and/or a specified frequency.
If input data is not available then the workflow is delayed till the data becomes available.
If no input data is needed then the workflow runs purely at a specified time or frequency.
Each of these workflows can have a coordinator.
A car is build with a collection of Coordinator jobs.
A collection of Coordinator jobs which cna be started, stopped and modified together is called a Bundle.
The output of one Coordinator job managing a workflow can be the input to another Coordinator job. 
--are chained together
Data Pipelines transform data in phases
Workflows, Coordinators, Bundlues all come together to form the building blocks of Oozie.

Oozie is an orchestration system for Hadoop jobs.

An Oozie Application defined by xml can be a workflow run manually, a single coordinator or a number of coordinators forming a bundle.

The applicaton can be a workflow run manually, a single coordinato or a number of coordinators forming
a bundle which is tipically representing a data pipeline

workflow.xml, coordinator.xml, bundle.xml

Oozie expects all files(including XML files) to be in HDFS before it can run

Messaging is one the popular trend in sharing the data between the applications/systems real time

There are two popular legacy messaging solutions Publish-Subscribe(Topic) and Queue:
Publish-Subscribe: messages are published to a message roker and message will be distributed to all of the consumers.
Topics retains message only as long as it tqakes to distribute them to current subscribers. The subscribers must
continue to be active for it to consume the messges.

Queues: Messages are published to a queue and the consumer will read from it. Limitatin is you can have only one
consumer per queue.

There is always a limit on the size of the message because larger message may end up breaking the message broker or
make the broker to perform slower.

Legacy Messaging solution have zero Fault-Tolerance.

What is Apache Kafka?
1 It is a scalable, reliable, high volume and high throughput distributed messaging system. It is maily used for sharing
high volumn data from one system to another system in real time and retention of data.

Advantages
1. Messages are not removed form the topic as soon as the consumers consume it.
2. Kafka is horizontally scalable
3. Kafka has stronger ordering guarantees than a traditional messaging system.
4. Kafka can handle high volume and it has very high throughput.
5. Kafka design supports loosely coupled Producers and Consumers.
6. Kafka can also be used as a storage system.


The componets that holds message in Kafka is called topics. Each topic has a unique name.
The topics live in broker, which is a server which can hold number of topics.

Kafka achieve high throughput and distribut management by Cluster, which is a group of Kafka brokers.
The Cluster will ditribute the load coming to it and process them simutaniously.

What is a Distributed System?
1. Systems are designed in such a way that it distributes the load within the system and process the load simultaneously.
2. To achieve simultaneous processing the load needs to be distributed across the cluster and there needs to be coordination
3.In world of distributed systems these are archieved using a protocal called gossip protocol
4. There need to be a system in place in order to monitor the









 



