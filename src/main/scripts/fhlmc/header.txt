Loan Identifier|Loan Correction Indicator|Prefix|Security Identifier|CUSIP|Mortgage Loan Amount|Issuance Investor Loan UPB|Current Investor Loan UPB|Amortization Type|Original Interest Rate|Issuance Interest Rate|Current Interest Rate|Issuance Net Interest Rate|Current Net Interest Rate|First Payment Date|Maturity Date|Loan Term|Remaining Months to Maturity|Loan Age|Loan-To-Value (LTV)|Combined Loan-To-Value (CLTV)|Debt-To-Income (DTI)|Borrower Credit Score|Filler|Filler|Filler|Number of Borrowers|First Time Home Buyer Indicator|Loan Purpose|Occupancy Status|Number of Units|Property Type|Channel|Property State|Seller Name|Servicer Name|Mortgage Insurance Percent|Mortgage Insurance Cancellation Indicator|Government Insured Guarantee|Assumability Indicator|Interest Only Loan Indicator|Interest Only First Principal and Interest Payment Date|Months to Amortization|Prepayment Penalty Indicator|Prepayment Penalty Total Term|Index|Mortgage Margin|MBS PC Margin|Interest Rate Adjustment Frequency|Interest Rate Lookback|Interest Rate Rounding Method|Interest Rate Rounding Method Percent|Convertibility Indicator|Initial Fixed Rate Period|Next Interest Rate Adjustment Date|Months to Next Interest Rate Adjustment Date|Life Ceiling Interest Rate|Life Ceiling Net Interest Rate|Life Floor Interest Rate|Life Floor Net Interest Rate|Initial Interest Rate Cap Up Percent|Initial Interest Rate Cap Down Percent|Periodic Interest Rate Cap Up Percent|Periodic Interest Rate Cap Down Percent|Modification Program|Modification Type|Number of Modifications|Total Capitalized Amount|Interest Bearing Mortgage Loan Amount|Original Deferred Amount|Current Deferred UPB|Loan Age As Of Modification|Estimated Loan-To-Value (ELTV)|Updated Credit Score|Filler|Interest Rate Step Indicator|Initial Step Fixed-Rate Period|Total Number of Steps|Number of Remaining Steps|Next Step Rate|Terminal Step Rate|Terminal Step Date|Step Rate Adjustment Frequency|Next Step Rate Adjustment Date|Months to Next Step Rate Adjustment Date|Periodic Step Cap Up Percent|Origination Mortgage Loan Amount|Origination Interest Rate|Origination Amortization Type|Origination Interest Only Loan Indicator|Origination First Payment Date|Origination Maturity Date|Origination Loan Term|Origination Loan-To-Value (LTV)|Origination Combined Loan-To-Value (CLTV)|Origination Debt-To-Income Ratio|Origination Credit Score|Filler|Filler|Filler|Origination Loan Purpose|Origination Occupancy Status|Origination Channel|Days Delinquent|Loan Performance History|Loan Participation Percent

create 'notifications' 'attributes', 'metrics'

#Column families are groups of columns which are usauly semantically related.
#When you crate a table in HBase, you don't have to specify the columns in the table.

# Every column has to belong to some column family.
# Every table must have at least 1 column family
# It is possible to add or change column families later, but this is rarely done. Hbase optimizes the storage based on Column family.

put 'notifications',2,'attributes:for_user','Chaz'
# An Hbase table is like a sorted map in which keys are row IDs and column IDs and the values are data itself.
put 'notifications',2,'attributes:type','Comment'

# update using put
# 1. inserting values for new keys(row id, column)
# 2. Updating the value for existing keys
put 'notifications',2,'metrics:open',0  --insert
put 'notifications',2,'metrics:open',1  --update
# The history of updates for a specific key is maintained and retrievable. Each version is stored with the created timestamp.
# During retrieval, the latest version is retrieved by default. 

# like a look up in Map, retrieving a row. Get operations only allow you to retrieve 1 row id at a time.
get 'notifications',2 --get a set of cell
# The default behavior is to return all the columns for the specified row id
# cell = value + timestamp. By default, the cell with the latest timestamp is retrieved
get 'notifications',2,'metrics:open'  --get one cell
get 'notifications',2,'metrics:open','attributes:type' --get list of cell

# Retrieving a range of row ids. HBase tables are sorted maps, ie. row ids are sorted
scan 'notifications'  --similar to select * from notifications returns all values
scan 'notifications',{COLUMNS=>['attributes:type'],LIMIT=>1,STARTROW=>"2"} --using dictionary to specify the options
# THe row id has to be passed as a string while insert it we use integer

delete 'notifications',2,'attributes:for_user'
# after delete all coulumns for row id 2, scan table cannot show row id 2.

disable 'notifications'
drop 'notifications'
#Before deleting a table, it must be disabled first.
# When a table is in use, HBase keeps an index of the row ids in memory. HBase also keeps a log of recent changes in 
# memory, which are periodically flushed to disk (files in HDFS)




git remote add origin https://fx36019@ybgweb.ny.ssmb.com/ybgit/scm/ybhbase/yb-apache-hbase.git
git push -u origin master

1. To create a table
a. HTableDescriptor specify the tablename and column families.
b. Connection connect to HBase
c. HBaseAdmin Create the table

2. Insert a row into table
a. All data is passed to HBase must be in the form of byte arrays, including row ids, column family names,
column names, values.

3. While a single put operation is atomic, a list of puts are not.


# create partition strategy
kite-dataset partition-config loan_identifier:copy as_of_date:copy -s fhlmc_monthly.avsc -o fhlmc_partition.json
# create mapping-config
kite-dataset mapping-config loan_identifier:key as_of_date:key \
loan_correction_indicator:loan \
prefix:loan \
security_identifier:loan \
cusip:loan \
mortgage_loan_amount:loan \
issuance_investor_loan_upb:loan \
current_investor_loan_upb:loan \
amortization_type:loan \
original_interest_rate:loan \
issuance_interest_rate:loan \
current_interest_rate:loan \
issuance_net_interest_rate:loan \
current_net_interest_rate:loan \
first_payment_date:loan \
maturity_date:loan \
loan_term:loan \
remaining_months_to_maturity:loan \
loan_age:loan \
ltv:loan \
cltv:loan \
dti:loan \
borrower_credit_score:loan \
f1:loan \
f2:loan \
f3:loan \
number_of_borrowers:loan \
first_time_home_buyer_indicator:loan \
loan_purpose:loan \
occupancy_status:loan \
number_of_units:loan \
property_type:loan \
channel:loan \
property_state:loan \
seller_name:loan \
servicer_name:loan \
mortgage_insurance_percent:loan \
mortgage_insurance_cancellation_indicator:loan \
government_insured_guarantee:loan \
assumability_indicator:loan \
interest_only_loan_indicator:loan \
interest_only_first_principal_and_interest_payment_date:loan \
months_to_amortization:loan \
prepayment_penalty_indicator:loan \
prepayment_penalty_total_term:loan \
index:arm_loan \
mortgage_margin:arm_loan \
mbs_pc_margin:arm_loan \
interest_rate_adjustment_frequency:arm_loan \
interest_rate_lookback:arm_loan \
interest_rate_rounding_method:arm_loan \
interest_rate_rounding_method_percent:arm_loan \
convertibility_indicator:arm_loan \
initial_fixed_rate_period:arm_loan \
next_interest_rate_adjustment_date:arm_loan \
months_to_next_interest_rate_adjustment_date:arm_loan \
life_ceiling_interest_rate:arm_loan \
life_ceiling_net_interest_rate:arm_loan \
life_floor_interest_rate:arm_loan \
life_floor_net_interest_rate:arm_loan \
initial_interest_rate_cap_up_percent:arm_loan \
initial_interest_rate_cap_down_percent:arm_loan \
periodic_interest_rate_cap_up_percent:arm_loan \
periodic_interest_rate_cap_down_percent:arm_loan \
modification_program:mod_loan \
modification_type:mod_loan \
number_of_modifications:mod_loan \
total_capitalized_amount:mod_loan \
interest_bearing_mortgage_loan_amount:mod_loan \
original_deferred_amount:mod_loan \
current_deferred_upb:mod_loan \
loan_age_as_of_modification:mod_loan \
eltv:mod_loan \
updated_credit_score:mod_loan \
f4:mod_loan \
interest_rate_step_indicator:mod_loan \
initial_step_fixed_rate_period:mod_loan \
total_number_of_steps:mod_loan \
number_of_remaining_steps:mod_loan \
next_step_rate:mod_loan \
terminal_step_rate:mod_loan \
terminal_step_date:mod_loan \
step_rate_adjustment_frequency:mod_loan \
next_step_rate_adjustment_date:mod_loan \
months_to_next_step_rate_adjustment_date:mod_loan \
periodic_step_cap_up_percent:mod_loan \
origination_mortgage_loan_amount:mod_loan \
origination_interest_rate:mod_loan \
origination_amortization_type:mod_loan \
origination_interest_only_loan_indicator:mod_loan \
origination_first_payment_date:mod_loan \
origination_maturity_date:mod_loan \
origination_loan_term:mod_loan \
origination_ltv:mod_loan \
origination_cltv:mod_loan \
origination_debt_to_income_ratio:mod_loan \
origination_credit_score:mod_loan \
f5:mod_loan \
f6:mod_loan \
f7:mod_loan \
origination_loan_purpose:mod_loan \
origination_occupancy_status:mod_loan \
origination_channel:mod_loan \
days_delinquent:mod_loan \
loan_performance_history:mod_loan \
   -s fhlmc_monthly.avsc -p fhlmc_partition.json -o fhlmc_mapping.json
# create dataset
 
kite-dataset create dataset:hbase:ybrdev93/fhlmc_daily -s fhlmc_monthly.avsc \
  --partition-by fhlmc_partition.json   --mapping fhlmc_mapping.json
  
kite-dataset create dataset:hbase:ybrdev93/fhlmc_daily -s fhlmc_monthly.avsc \
  --partition-by fhlmc_partition.json   --mapping fhlmc_mapping.json


kite-dataset create dataset:hive://ybgdev93:9083/new/fhlmc_daily --schema fhlmc_monthly.avsc \
 --partition-by fhlmc_partition.json --mapping fhlmc_mapping.json

  
# import data

kite-dataset csv-import  FHLMONLF_TS.dat  dataset:hbase:ybrdev93/fhlmcdaily

kite-dataset delete dataset:hbase:ybrdev93/fhlmcdaily

kite-dataset info dataset:hbase:ybrdev93/fhlmcdaily

1. HBase is a column oriented storage. A sorted nested map.
<Row id, 
	ColumnFamily,
		<Column,
			<Timestamp, Value>>>
2. Row ids in a table are divided into ranges called regions. Each region is handled by a Region Server.
3. Regions server as an index to perform fast lookup for where a row key belongs.
4. Initially all writes are stored in memory called Memstore and a change log is written to disk called
   WriteAheadLog, which is created for recovery in case the Region Server crashes. Periodically the MemStore
   gets fall and the data in Memstore is flushed into HFile. HDFS will break the HFIle into blocks and store
   in in different nodes.
5. To minimize disk seeks, the region server keeps an index of row key to HFile block in memory.
6. HBase uses a Master server to manage Regions and RegionServers. The Master assigns regions to 
   Region Servers, managers load balancing etc.
7. The Master users Zoopkeeper to help assign regions to region servers.
   
 Any data processing task or many data processing tasks can be parallelized, if you express it as
 <key, value> --> map() --> <key, Value> -->reduce() --><key, value> 
 or a chain of such transformations.
 
 A map() task that transforms a single key, value pair to a set of key, value pairs. 
 A reduce() task combines values which have the same key.
 
 We realized that most of our computations involved applying a map operation to each logical record where
 the record is basically a key value pair.
 
 So you apply a map and then you compute a set of intermediate key/value pairs, which is the output of
 map which servers as an input for the next transformation in the chain which is the reduce operation.
 
 Then they applied reduce operations to all values which share the same key, in order to combine
 the derived data.
 
 The algorithm for Map, Reduce steps needs to be defined by the user.The Sort/Merge step is taken care of by Hadoop.
 
 The map function will run once for each line of the test file. THe inputs and outputs are both key/value pair.
 So the operation can run in parallel on each data node. There is no interdependency in the inputs
 and outputs
 
 All of these results are first copied over to a single node.
 
 The reduce() will run on each pair generated by the Sort/Merge step. The key is unique. The output is sorted.
 
 <row id, <col, value>> --Map--> <type, 1>\
 TableMapper is a special subclass of the Mapper class that can read from HBase tables.
 
 <type, 1> --Reduce--> <null, Put>
 TableReduce is a spcial subclass of the Reducer class that can write to HBase tables
 
 Step 1: Write a map() function --TableMapper
 Step 2: Write a reduce() function --TableReducer
 Step 3: Set up a driver that points to our map and reduce implementations 
 
 The output types of the Mapper should match the input types of the Reducer
 Hadoop knows the input parameter for map() is  <row id, <col, value>> 
 Hadoop knows the output parameter for reduce is key=null. value=Put object
 Hadoop users the ImmutableBytesWritable (row id) and Result classes (<col,value>)to represent keys, values in HBase tables.

TableMapper class and TableReducer class, 2 classes are used by a Job that is configured in the Main class.
THe Job has bunch of properties that need to be confgured, etc. Input table, output table, Mapper Class, Reducer class

hadoop jar /root/.m2/repository/com/yieldbook/HBaseJava/2.0.0/HBaseJava-2.0.0-shaded.jar com.yieldbook.mortgage.hbase.mapReduce.Main
 
 
 
 
 
 
 

  

