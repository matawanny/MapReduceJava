1. Bulk import csv to Hbase

create 'wordcount', {NAME => 'f'},   {SPLITS => ['g', 'm', 'r', 'w']}

export HADOOP_CLASSPATH=$SQOOP_HOME/lib/jconn4.jar:$SQOOP_HOME/lib/jTDS3.jar:$HIVE_HOME/lib/*:$HBASE_HOME/lib/*:$HBASE_HOME/conf:$HBASE_HOME/


hadoop jar ./lib/hbase/hbase-0.94.6-cdh4.3.0-security.jar importtsv \
-Dimporttsv.separator="," \
-Dimporttsv.bulk.output=/user/root/source/output \
-Dimporttsv.columns=HBASE_ROW_KEY,f:count wordcount /user/root/source/word_count.cs

hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /user/root/source/output wordcount



CREATE EXTERNAL TABLE wordcount(keyword string, count string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,f:count","hbase.table.default.storage.type" = "binary")

2.

create 'test', {NAME=>'m'}, {SPLITS => ['row1', 'row6']}


hadoop jar $HBASE_HOME/bulk.jar com.cloudera.examples.hbase.bulkimport.Driver <hdfsDirectory>/file1.txt \ <hdfsDirectory>/<outputname>/ <tablename>

3. The goal of Combiner is to minium key value paires that will be shuffled across network between mappers and reducers.
THe combiner is also known as Mini-reducer. 
It should be used where the nature of problem is Associative and Commutative 
Commutative A+B=B+A
Associative (A+B)+C = A+(B+C)

Combiner can run any number of times

Combiner does this by performing the same reducer task in the Maps locally. Combiner and Reducer use the same code.

create 'fhlmc_loan', {NAME => 'm'},   {SPLITS => ['g', 'm', 'r', 'w']}



echo never>/sys/kernel/mm/transparent_hugepage/defrag
echo never>/sys/kernel/mm/transparent_hugepage/enabled
export TMOUT=36000
export HISTFILESIZE=1000
export HISTSIZE=1000
#export JAVA_HOME=/usr/java/jdk1.8.0_131
#export JRE_HOME=/usr/java/jdk1.8.0_131/jre
export JAVA_HOME=/usr/lib/jvm/jre-1.7.0-openjdk.x86_64/
export JAVA_TOOL_OPTIONS="-Xss2m"
export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export HADOOP_PREFIX=/opt/cloudera/parcels/CDH/lib/hadoop
export HIVE_HOME=/opt/cloudera/parcels/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hive
export SQOOP_HOME=/opt/cloudera/parcels/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/sqoop
export HBASE_HOME=/opt/cloudera/parcels/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hbase
export HADOOP_CLASSPATH=$SQOOP_HOME/lib/jconn4.jar:$SQOOP_HOME/lib/jTDS3.jar:$HIVE_HOME/lib/*:$HBASE_HOME/lib/*:$HBASE_HOME/conf:$HBASE_HOME/
export SQOOP_CLASSPAT=$SQOOP_HOME/lib/jconn4.jar:$SQOOP_HOME/lib/jTDS3.jar
export M2_HOME=/usr/book/opt/apache-maven-3.5.3
export PATH=$PATH:$JAVA_HOME/bin:$M2_HOME/bin:$HADOOP_HOME/bin
export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:$HIVE_HOME/lib/*:$HBASE_HOME/lib/*:$HBASE_HOME/:.
unset SSH_ASKPASS



useradd zookeeper
passwd zookeeper
useradd -d /var/lib/zookeeper zookeeper
cat/etc/passwd

groupadd zookeeper
cat /etc/group

ueradd -g "zookeeper" zookeeper

edit .bashrc and .bash_profile
echo never>/sys/kernel/mm/transparent_hugepage/defrag
echo never>/sys/kernel/mm/transparent_hugepage/enabled
export TMOUT=36000
export HISTFILESIZE=1000
export HISTSIZE=1000
export JAVA_HOME=/usr/java/jdk1.8.0_131
export JRE_HOME=/usr/java/jdk1.8.0_131/jre
export JAVA_TOOL_OPTIONS="-Xss2m"
export HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop
export HADOOP_PREFIX=/opt/cloudera/parcels/CDH/lib/hadoop
export HIVE_HOME=/opt/cloudera/parcels/CDH/lib/hive
export SQOOP_HOME=/opt/cloudera/parcels/CDH/lib/sqoop
export HBASE_HOME=/opt/cloudera/parcels/CDH/lib/hbase
export HADOOP_CLASSPATH=$SQOOP_HOME/lib/jconn4.jar:$SQOOP_HOME/lib/jTDS3.jar:$HIVE_HOME/lib/*:$HBASE_HOME/lib/*:$HBASE_HOME/conf:$HBASE_HOME/*
export SQOOP_CLASSPAT=$SQOOP_HOME/lib/jconn4.jar:$SQOOP_HOME/lib/jTDS3.jar
export M2_HOME=/usr/book/opt/apache-maven-3.5.3
export PATH=$PATH:$JAVA_HOME/bin:$M2_HOME/bin:$HADOOP_HOME/bin
export CLASSPATH=$CLASSPATH:$HADOOP_HOME/lib/*:$HIVE_HOME/lib/*:$HBASE_HOME/lib/*:$HBASE_HOME/*:.
export ZK_HOME=/var/lib/zookeeper


____________________________________

export CLASSPATH=/usr/share/java/mysql-connector-java.jar:$CLASSPATH

HBase provides a large number of build in filters:
a. Specific a set of row ids
b. Specific set of columns/column families
c. Specific value for a column
d. Timestamp and more...

create 'fhlmc_loan_hbase', {NAME =>'m'}, {SPLITS => ['C05084008226', 'J15433000241', 'Q20826000030', 'V60172000602','V84230000010']}
______________________________

create 'fhlmc_loan_monthly', {NAME =>'m'}
create 'fhlmc_arm_loan_monthly', {NAME =>'m'}
create 'fhlmc_mod_loan_monthly', {NAME =>'m'}

create 'fnma_loan_monthly', {NAME =>'m'}
create 'fnma_arm_loan_monthly', {NAME =>'m'}
create 'fnma_mod_loan_monthly', {NAME =>'m'}
